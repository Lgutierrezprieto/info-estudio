{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction  \n",
    "In this lab, we would like to make an XGBoost model to study the e-commerce behavior from a multi-category store. First, we need to download the data to your local machine, then we will load the data into a Pandas DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "- Apply XGBoost to an example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instruction\n",
    "* Accept the Kaggle policy and download the data from [Kaggle](https://www.kaggle.com/code/tshephisho/ecommerce-behaviour-using-xgboost/data)\n",
    "* For the first model you will only use the 2019-Nov csv data (which is still around ~2gb zipped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as dates\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "# fill this in with your path (absolute path works as well)\n",
    "path_to_file = \" \"\n",
    "df = pd.read_csv(path_to_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start with some exploratory analysis. First, take a look at the first five rows of the DataFrame. Then get the information about the DataFrame, what is the shape of the DataFrame, and what are the coumn names?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Know your Customers\n",
    "How many unique customers visit the site? Assign the number of visitors to the visitor variable and print it out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code\n",
    "visitor = df[\"user_id\"].nunique()\n",
    "print(\"visitors: {}\".format(visitor))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visitors Daily Trend\n",
    "Does traffic fluctuate by date? Try using the `event_time` and `user_id` to see traffic. First you need to select by `event_time` and `user_id`, then you will `drop_duplicates` and `groupby` `event_time` and `user_id`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = df.loc[:, [\"event_time\", \"user_id\"]]\n",
    "d[\"event_time\"] = d[\"event_time\"].apply(lambda s: str(s)[0:10])\n",
    "visitor_by_date = (\n",
    "    d.drop_duplicates()\n",
    "    .groupby([\"event_time\"])[\"user_id\"]\n",
    "    .agg([\"count\"])\n",
    "    .sort_values(by=[\"event_time\"], ascending=True)\n",
    ")\n",
    "x = pd.Series(visitor_by_date.index.values).apply(\n",
    "    lambda s: datetime.strptime(s, \"%Y-%m-%d\").date()\n",
    ")\n",
    "y = visitor_by_date[\"count\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (17, 5)\n",
    "plt.plot(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### By Category and Product\n",
    "Which category do customers interact with the most? What brand do they view the most? You can choose just the categories with at least 30 records in order to construct the plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_category_num = 30\n",
    "top_category = (\n",
    "    df.loc[:, \"category_code\"]\n",
    "    .value_counts()[:max_category_num]\n",
    "    .sort_values(ascending=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(\n",
    "    height=top_category,\n",
    "    x=top_category.index.array,\n",
    "    color=[\"red\", \"cyan\", \"green\", \"orange\", \"blue\", \"grey\"],\n",
    "    alpha=0.7,\n",
    ")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Purchases\n",
    "\n",
    "When the event_type is \"purchase\", what item do customers buy?\n",
    "\n",
    "Try using `'event_type' == 'purchase'` and drop empty rows to assess which categories customers buy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code\n",
    "purchase = df.loc[df[\"event_type\"] == \"purchase\"]\n",
    "purchase = purchase.dropna(axis=\"rows\")\n",
    "purchase.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What brands do the customers buy?\n",
    "Try grouping by brand and sorting the values by the brand name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code\n",
    "top_sellers = (\n",
    "    purchase.groupby(\"brand\")[\"brand\"]\n",
    "    .agg([\"count\"])\n",
    "    .sort_values(\"count\", ascending=False)\n",
    ")\n",
    "top_sellers.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del d  # free memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling: predict at the time of addition to a shopping cart if the user will purchase a given product or not\n",
    "\n",
    "### Feature engineering\n",
    "\n",
    "The goal of this modeling is to predict if the user will purchase a product or not when they add the product to the cart. This is called `cart abandonment` if the user does not purchase.\n",
    "\n",
    "First, reconstruct and restructure the data to feed into the machine learning model. For this use case, target only the data for which customers have \"put\" the product into the cart. The relevant `event_type`s are thus \"cart\" and \"purchase\".\n",
    "\n",
    "Create these new features in the training data set:\n",
    "- `activity_count`: number of activity in that session\n",
    "- `category_level1`: category\n",
    "- `category_level2`: sub-category --> split on the \".\" in the category name\n",
    "- `weekday`: weekday of the event --> convert `event_time` to a datetime object, then use `pandas.Timestamp.weekday`\n",
    "- `is_purchased`: whether the is purchased after being put in the cart, this will be the categorical output.\n",
    "\n",
    "Make sure to de-dup any record."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prepare a dataframe for counting activity in the session**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code\n",
    "# first just eliminate the records where event_type = \"view\" and drop NA values and duplicates\n",
    "# your code\n",
    "# first just eliminate the records where event_type = \"view\" and drop NA values and duplicates\n",
    "\n",
    "df_targets = df.loc[df[\"event_type\"].isin([\"cart\", \"purchase\"])].drop_duplicates(\n",
    "    subset=[\"event_type\", \"product_id\", \"price\", \"user_id\", \"user_session\"]\n",
    ")\n",
    "cart_purchase_users = df.loc[\n",
    "    df[\"event_type\"].isin([\"cart\", \"purchase\"])\n",
    "].drop_duplicates(subset=[\"user_id\"])\n",
    "cart_purchase_users.dropna(how=\"any\", inplace=True)\n",
    "cart_purchase_users_all_activity = df.loc[\n",
    "    df[\"user_id\"].isin(cart_purchase_users[\"user_id\"])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now you get the number of activities by user session\n",
    "\n",
    "activity_in_session = (\n",
    "    cart_purchase_users_all_activity.groupby([\"user_session\"])[\"event_type\"]\n",
    "    .count()\n",
    "    .reset_index()\n",
    ")\n",
    "activity_in_session = activity_in_session.rename(\n",
    "    columns={\"event_type\": \"activity_count\"}\n",
    ")\n",
    "df_targets = cart_purchase_users_all_activity.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the two new columns for the category levels 1 and 2\n",
    "# your code here\n",
    "df_targets[\"category_level1\"] = (\n",
    "    df_targets[\"category_code\"].str.split(\".\", expand=True)[0].astype(\"category\")\n",
    ")\n",
    "df_targets[\"category_level2\"] = (\n",
    "    df_targets[\"category_code\"].str.split(\".\", expand=True)[1].astype(\"category\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the event_time to a timestamp\n",
    "# your code\n",
    "df_targets[\"timestamp\"] = pd.to_datetime(df_targets[\"event_time\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use pandas.dt.dayofweek to get the day of the week\n",
    "# your code\n",
    "df_targets[\"weekday\"] = df_targets[\"timestamp\"].dt.dayofweek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the is_purchased feature\n",
    "# your code\n",
    "df_targets[\"is_purchased\"] = np.where(df_targets[\"event_type\"] == \"purchase\", 1, 0)\n",
    "df_targets[\"is_purchased\"] = df_targets.groupby([\"user_session\", \"product_id\"])[\n",
    "    \"is_purchased\"\n",
    "].transform(\"max\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_targets = df_targets.merge(activity_in_session, on=\"user_session\", how=\"left\")\n",
    "df_targets[\"activity_count\"] = df_targets[\"activity_count\"].fillna(0)\n",
    "df_targets[\"brand\"] = df_targets[\"brand\"].astype(\"category\")\n",
    "df_targets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save new data structure if desired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_targets.to_csv('training_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_targets.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import plot_importance\n",
    "from sklearn.utils import resample\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resampling training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_purcahase_set = df_targets[df_targets[\"is_purchased\"] == 1]\n",
    "is_purcahase_set.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_purcahase_set = df_targets[df_targets[\"is_purchased\"] == 0]\n",
    "not_purcahase_set.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 500000\n",
    "is_purchase_downsampled = resample(\n",
    "    is_purcahase_set, replace=False, n_samples=n_samples, random_state=27\n",
    ")\n",
    "not_purcahase_set_downsampled = resample(\n",
    "    not_purcahase_set, replace=False, n_samples=n_samples, random_state=27\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downsampled = pd.concat([is_purchase_downsampled, not_purcahase_set_downsampled])\n",
    "downsampled[\"is_purchased\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the brand, price, weekday, category_level1, cateogry_level2, and activity_count features\n",
    "\n",
    "# select the brand, price, weekday, category_level1, cateogry_level2, and activity_count features\n",
    "\n",
    "features = downsampled.loc[\n",
    "    :,\n",
    "    [\n",
    "        \"brand\",\n",
    "        \"price\",\n",
    "        \"weekday\",\n",
    "        \"category_level1\",\n",
    "        \"category_level2\",\n",
    "        \"activity_count\",\n",
    "    ],\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.loc[:, \"brand\"] = LabelEncoder().fit_transform(\n",
    "    downsampled.loc[:, \"brand\"].copy()\n",
    ")\n",
    "features.loc[:, \"weekday\"] = LabelEncoder().fit_transform(\n",
    "    downsampled.loc[:, \"weekday\"].copy()\n",
    ")\n",
    "features.loc[:, \"category_level1\"] = LabelEncoder().fit_transform(\n",
    "    downsampled.loc[:, \"category_level1\"].copy()\n",
    ")\n",
    "features.loc[:, \"category_level2\"] = LabelEncoder().fit_transform(\n",
    "    downsampled.loc[:, \"category_level2\"].copy()\n",
    ")\n",
    "\n",
    "is_purchased = LabelEncoder().fit_transform(downsampled[\"is_purchased\"])\n",
    "features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(features.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the data\n",
    "Use a test size of 0.3 and a random state of 86 to split the data into test and train subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    features, is_purchased, test_size=0.3, random_state=86\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "Choose learning rate of 0.1 on XGBClassifier, fit the model, and make predictions on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "model = XGBClassifier(learning_rate=0.1)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"Precision:\", metrics.precision_score(y_test, y_pred))\n",
    "print(\"Recall:\", metrics.recall_score(y_test, y_pred))\n",
    "print(\"fbeta:\", metrics.fbeta_score(y_test, y_pred, average=\"weighted\", beta=0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature importance\n",
    "Plot the feature importance using plot_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_importance(model, max_num_features=10, importance_type=\"gain\")\n",
    "# plt.rcParams['figure.figsize'] = (40,10)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "saturn (Python 3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
